{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56883dd7",
   "metadata": {},
   "source": [
    "# **Credit Card Customer Churn Analysis (ML)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e855d3",
   "metadata": {},
   "source": [
    "## Objectives – Machine Learning (ML)\n",
    "\n",
    "In this notebook, I will build, evaluate, and interpret a predictive model for customer churn using the processed Credit Card Customer Churn dataset. The goal is to leverage insights from the EDA notebook to select relevant features, train a logistic regression model, and assess which customer attributes significantly influence churn probability.\n",
    "\n",
    "This notebook focuses on:\n",
    "\n",
    "* Preparing data for modeling, including feature selection, scaling, and train/test splitting.\n",
    "* Building an interpretable Logistic Regression model to predict churn (binary classification).\n",
    "* Evaluating model performance using metrics such as Accuracy, Precision, Recall, F1-score, and ROC-AUC.\n",
    "* Visualizing feature importance to identify which factors drive churn.\n",
    "* Translating model results into actionable insights for business stakeholders.\n",
    "\n",
    "### **Inputs**\n",
    "\n",
    "To run this notebook, the following inputs are required:\n",
    "\n",
    "* Processed dataset CSV: The cleaned and transformed Credit Card Customer Churn dataset produced in the ETL notebook.\n",
    "* Python libraries: Including but not limited to:\n",
    "* pandas – data manipulation\n",
    "* numpy – numerical operations\n",
    "* scikit-learn – machine learning algorithms, scaling, train/test split, evaluation metrics\n",
    "* matplotlib and seaborn – visualizations for performance metrics and feature importance\n",
    "\n",
    "### **Outputs**\n",
    "\n",
    "This notebook will generate:\n",
    "\n",
    "* Train/Test Sets: Split datasets ready for modeling.\n",
    "* Scaled Features: Normalized numeric features for better model performance.\n",
    "* Model Performance Metrics: Accuracy, Precision, Recall, F1-score, ROC-AUC, and confusion matrix visualizations.\n",
    "* Feature Importance Visualization: Bar charts of logistic regression coefficients showing positive or negative influence on churn.\n",
    "* Interpretation of Results: Insights on which features support or refute your original hypotheses (H1–H3).\n",
    "* Optional Export: DataFrame with predicted churn probabilities for downstream dashboard integration or further analysis.\n",
    "\n",
    "### **Workflow**\n",
    "\n",
    "**1. Data Preparation**\n",
    "\n",
    "* Load the processed dataset.\n",
    "* Separate features (X) and target (y).\n",
    "* Scale numeric features.\n",
    "* Encode categorical variables if needed (already one-hot in your dataset).\n",
    "\n",
    "**2. Train/Test Split**\n",
    "\n",
    "* Split the dataset into training and testing sets (e.g., 80/20).\n",
    "* Ensure stratification on the target variable (Attrition_Flag) to preserve class balance.\n",
    "\n",
    "**3. Model Training**\n",
    "\n",
    "* Train a Logistic Regression model.\n",
    "* Optionally, compare with other interpretable models (e.g., Random Forest for feature importance).\n",
    "\n",
    "**4. Model Evaluation**\n",
    "\n",
    "* Evaluate predictions on the test set using standard metrics.\n",
    "* Visualize results via confusion matrix and ROC curve.\n",
    "\n",
    "**5. Feature Interpretation**\n",
    "\n",
    "* Extract model coefficients.\n",
    "* Rank features by absolute influence on churn probability.\n",
    "* Visualize top features using bar charts for easy stakeholder interpretation.\n",
    "\n",
    "**6. Hypothesis Insights**\n",
    "\n",
    "* Map the top features back to H1–H3:\n",
    "* H1 – Tenure & Age\n",
    "* H2 – Credit usage & transactions\n",
    "* H3 – Income & card category\n",
    "* Summarize which hypotheses are supported or contradicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4a16e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML & preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Style\n",
    "sns.set_style(\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc3dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
